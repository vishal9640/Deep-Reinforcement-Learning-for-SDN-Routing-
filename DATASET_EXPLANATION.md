# expert_dataset.npz - Complete Explanation

## File Overview

**Filename:** `expert_dataset.npz`  
**File Size:** 43.47 KB  
**Format:** NumPy compressed archive (.npz)  
**Created By:** `create_sample_dataset.py` (synthetic generator for quick testing)

---

## What's Inside

### 1. **STATES Array** (500 samples × 12 features)
```
Shape: (500, 12)
Data Type: float32
Storage: ~2 KB
```

**What it represents:**
- 500 snapshots of the network state
- Each snapshot has 12 numeric features

**Feature Breakdown (12 columns):**
```
Indices 0-9    → Link utilization (10 links in network topology)
Index 10       → Network delay (normalized)
Index 11       → Network throughput (normalized)
```

**Current values:** All zeros (synthetic dummy data for testing)
```
State[0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
State[1]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
State[2]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
...
```

**In real usage:**
- Values range from 0.0 to 1.0 (normalized)
- Comes from querying Ryu REST API `/stats` endpoint
- Generated by `sdn_env.py` when collecting expert demonstrations

---

### 2. **ACTIONS Array** (500 samples × 10 actions)
```
Shape: (500, 10)
Data Type: float32
Storage: ~2 KB
```

**What it represents:**
- Action taken by expert policy at each state
- 10 values = one weight delta per link in topology

**Feature Breakdown (10 columns):**
```
Each column → Link weight adjustment for that link
Range → [-1.0, +1.0] (increase or decrease weight)
```

**Current values:** All zeros (expert follows "no-change" policy for testing)
```
Action[0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
Action[1]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
Action[2]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
...
```

**In real usage:**
- Values computed by expert policy (e.g., equal-cost multipath routing)
- Or collected from real human operator decisions
- Sent to Ryu via REST API `/set_link_weights`

---

### 3. **REWARDS Array** (NOT stored, computed at training time)
```
Currently: Absent from .npz
Reason: Synthetic data doesn't have real rewards
When needed: Computed by training code from states
```

**How it's computed:**
```python
# Inside envs/offline_env.py
reward = throughput_normalized - delay_normalized - max(link_utils)
# Range: approximately [-1, +1]
```

---

## How It Works in the ML Pipeline

### **Pipeline Step 1: Imitation Pretraining**
```
Input:    expert_dataset.npz (states + actions)
Process:  Actor network learns to mimic actions from states
Loss:     MSE Loss: actor(state) vs. target_action
Output:   models/actor_pretrained.pth (trained weights)

Command:
  python pretrain/imitation_pretrain.py \
    --data expert_dataset.npz \
    --epochs 50 \
    --batch_size 32
```

**What happens:**
1. Load states (500×12) and actions (500×10)
2. Split into batches of 32 samples
3. Pass states through Actor network → predicted_actions (500×10)
4. Compute MSE loss between predicted_actions and target_actions
5. Backprop to update Actor weights
6. Save trained weights to `models/actor_pretrained.pth`

**Result:** Actor network now can generate actions similar to expert policy

---

### **Pipeline Step 2: Offline RL Training**
```
Input:    expert_dataset.npz (via envs/offline_env.py)
Process:  DDPG agent learns via offline reinforcement learning
Compute:  Rewards from states (since not in .npz)
Output:   models/ddpg_best.pth (best trained agent)

Command:
  python train/train_ddpg.py \
    --offline_data expert_dataset.npz \
    --pretrained_actor models/actor_pretrained.pth \
    --episodes 100 \
    --max_steps_per_episode 50
```

**What happens:**
1. Load states from .npz into OfflineSDNEnv
2. For each episode, step through stored states sequentially:
   - Get state_t from buffer
   - Actor generates action from state_t
   - Get state_t+1 from buffer (next state)
   - Compute reward: `reward = throughput - delay - link_util`
   - Store (state_t, action, reward, state_t+1) in replay buffer
3. Trainer:
   - Samples batch from replay buffer
   - Updates Critic network: minimize TD error
   - Updates Actor network: maximize Q-value
4. Save best checkpoint to `models/ddpg_best.pth`

**Result:** DDPG agent fine-tuned to optimize the reward function

---

### **Pipeline Step 3: Live Training (Optional)**
```
Input:    Live Ryu/Mininet environment + pretrained actor
Process:  Agent fine-tunes on real network dynamics
Output:   models/ddpg_best.pth (final trained agent)

Command (on Linux with Mininet):
  sudo python topologies/simple_topo.py  # terminal 1: Start topology
  cd ryu && ryu-manager ...              # terminal 2: Start Ryu
  python train/train_ddpg.py \
    --pretrained_actor models/actor_pretrained.pth \
    --episodes 100
```

**What happens:**
1. Skip expert_dataset.npz entirely
2. Instead, interact with live SDNEnv (queries Ryu REST API)
3. Collect real (state, action, reward, next_state) transitions
4. Same training loop as offline, but with real network data
5. Agent learns real network dynamics, not synthetic

---

## Data Flow Diagram

```
expert_dataset.npz (synthetic or real)
    |
    ├─→ [Imitation Pretraining]
    |      Input: states, actions
    |      Output: actor_pretrained.pth
    |
    ├─→ [Offline RL Training] (uses envs/offline_env.py)
    |      Input: states + computed rewards
    |      Output: ddpg_best.pth
    |
    └─→ [Live RL Training] (optional, ignores .npz)
           Input: Real Ryu/Mininet environment
           Output: ddpg_best.pth (refined)

Final Agent: ddpg_best.pth
    |
    └─→ [Inference/Evaluation] (run_inference.py)
           Input: Test environment (offline or live)
           Output: Episode rewards, actions taken
```

---

## Topology-Dataset Mapping

### Current Dataset Info
- **10 Links** in the network (inferred from action dimension = 10)
- **Link Count = 10** (derived from states shape: 12 - 2 = 10)
- **Topology:** Corresponds to the 2-switch configuration in `topologies/simple_topo.py`

### Topology Definition
```python
# topologies/simple_topo.py

Hosts:    h1, h2, h3, h4
Switches: s1, s2
Links:
  1. h1 ↔ s1
  2. h2 ↔ s1
  3. h3 ↔ s2
  4. h4 ↔ s2
  5. s1 ↔ s2 (inter-switch)
  
Total: 5 physical links → represented as 10 directed links
(each undirected link = 2 directions for traffic flows)
```

### **IMPORTANT:** Dataset-Topology Coupling
- Your current dataset expects **exactly 10 links** in state space
- Your model is trained on **10-dimensional action space**
- If you change topology → link count changes → data becomes **incompatible**

**Solution:** Always run training and inference with matching topology
```bash
# Dataset expects 10 links
python train/train_ddpg.py --offline_data expert_dataset.npz
# Must use model expecting 10 actions

# Never mix different topologies:
# DON'T do: Train on 10-link dataset, test on 6-link topology
```

---

## Real vs. Synthetic Data

### **Synthetic Data (Current: expert_dataset.npz)**
```
States:      All zeros
Actions:     All zeros (baseline policy: no changes)
Rewards:     Not stored (will be zero when computed)
Use Case:    Testing pipeline, quick iteration
Reality:     Unrealistic - no network dynamics
```

**Pros:**
- ✓ Fast to generate (instant)
- ✓ Good for checking code runs without errors
- ✓ No Ryu/Mininet needed

**Cons:**
- ✗ Agent learns nothing meaningful
- ✗ All rewards are zero
- ✗ No real network patterns

---

### **Real Data (from Ryu+Mininet)**
```
States:      Actual link utilizations [0-1], delays, throughputs
Actions:     Expert policy decisions (tuned weights)
Rewards:     Real QoS metrics (throughput/delay tradeoff)
Use Case:    Actual ML training
Reality:     Captures network behavior, congestion patterns
```

**Pros:**
- ✓ Agent learns realistic behavior
- ✓ Transferable to real networks
- ✓ Meaningful training rewards

**Cons:**
- ✗ Slow to collect (2+ hours for 100 episodes)
- ✗ Requires Ryu + Mininet (Linux only)
- ✗ Depends on traffic patterns used

---

## How to Verify Dataset Contents

### Quick Python Check
```python
import numpy as np

data = np.load('expert_dataset.npz')

# See all keys
print(data.files)  # ['states', 'actions']

# Inspect shapes
print(data['states'].shape)   # (500, 12)
print(data['actions'].shape)  # (500, 10)

# Inspect values
print(data['states'][0])      # [0. 0. 0. ...]
print(data['actions'][:5])    # First 5 action vectors

# Check data ranges
print(data['states'].min(), data['states'].max())
print(data['actions'].min(), data['actions'].max())
```

---

## Integration with Your Code

### In `pretrain/imitation_pretrain.py`
```python
data = np.load(expert_dataset_path)
states = data['states']      # (500, 12)
actions = data['actions']    # (500, 10)
rewards = data.get('rewards', None)  # None (not stored)

# Train actor to predict actions from states
for epoch in range(epochs):
    for batch in batches:
        predicted_actions = actor(batch['states'])
        loss = MSE(predicted_actions, batch['actions'])
        loss.backward()
        optimizer.step()
```

### In `envs/offline_env.py`
```python
class OfflineSDNEnv:
    def __init__(self, data_path):
        self.data = np.load(data_path)
        self.states = self.data['states']       # (500, 12)
        self.actions = self.data['actions']     # (500, 10)
        self.rewards = self.data.get('rewards', None)
        self.idx = 0
    
    def step(self, action):
        state = self.states[self.idx]
        next_state = self.states[self.idx + 1] if self.idx < len(self.states)-1 else state
        
        # Compute reward if not in dataset
        if self.rewards is None:
            reward = self._compute_reward(state)  # Uses formula
        else:
            reward = self.rewards[self.idx]
        
        self.idx += 1
        return next_state, reward, False, {}
```

### In `train/train_ddpg.py`
```python
# Load offline data
if args.offline_data:
    from envs.offline_env import OfflineSDNEnv
    env = OfflineSDNEnv(args.offline_data)
else:
    # Use live Ryu environment
    from sdn_env import SDNEnv
    env = SDNEnv(...)

# Pretrain agent from expert data (optional)
if args.pretrained_actor:
    agent.actor.load_state_dict(torch.load(args.pretrained_actor))
    agent.actor_target.load_state_dict(torch.load(args.pretrained_actor))
```

---

## Summary Table

| Aspect | States | Actions | Rewards |
|--------|--------|---------|---------|
| **Shape** | (500, 12) | (500, 10) | Not stored |
| **Data Type** | float32 | float32 | — |
| **Range** | [0, 1] (normalized) | [-1, 1] (deltas) | — |
| **Meaning** | Network metrics per link | Link weight adjustments | — |
| **Storage** | ~2 KB | ~2 KB | 0 bytes |
| **Current Values** | All zeros | All zeros | Computed at train time |
| **Source** | Synthetic (offline_env) | Synthetic (offline_env) | Formula: `throughput - delay - link_util` |

---

## Next Steps

1. **To use this dataset for training:**
   ```bash
   # Step 1: Imitation pretraining
   python pretrain/imitation_pretrain.py \
     --data expert_dataset.npz \
     --epochs 50

   # Step 2: Offline RL training
   python train/train_ddpg.py \
     --offline_data expert_dataset.npz \
     --pretrained_actor models/actor_pretrained.pth \
     --episodes 100

   # Step 3: Evaluate trained agent
   python run_inference.py \
     --model models/ddpg_best.pth \
     --offline_data expert_dataset.npz \
     --episodes 10
   ```

2. **To collect real data (on Linux with Mininet):**
   ```bash
   python scripts/generate_expert_dataset.py \
     --ryu_ip 127.0.0.1 \
     --ryu_port 8080 \
     --episodes 100 \
     --output real_dataset.npz
   ```

3. **To visualize dataset statistics:**
   ```python
   import numpy as np
   import matplotlib.pyplot as plt
   
   data = np.load('expert_dataset.npz')
   states = data['states']
   
   # Histogram of link utilizations
   plt.hist(states[:, :10].flatten(), bins=50)
   plt.xlabel('Link Utilization')
   plt.ylabel('Frequency')
   plt.show()
   ```

